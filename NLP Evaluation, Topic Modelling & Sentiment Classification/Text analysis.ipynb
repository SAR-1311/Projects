{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616611d4-e9c6-4bd6-805f-b646f49122ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a152755-13ed-4756-b720-1b3a9eaadfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc6ce0ca-e680-4c96-8923-98035595896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"bible_and_quran.tsv\"   \n",
    "STOPWORDS_PATH = \"stopwords.txt\"    \n",
    "N_TOPICS = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3d362a1-5819-4d22-8fb2-c51413df63fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 570 custom stopwords from stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords(path: str) -> set:\n",
    "    stopwords_set = set()\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            w = line.strip().lower()\n",
    "            if w:\n",
    "                stopwords_set.add(w)\n",
    "    return stopwords_set\n",
    "\n",
    "\n",
    "stop_words = load_stopwords(STOPWORDS_PATH)\n",
    "print(f\"Loaded {len(stop_words)} custom stopwords from {STOPWORDS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42aba513-4418-4096-adc8-4709bc1af7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (30367, 2)\n",
      "Example rows:\n",
      "  corpus                                               text\n",
      "0     OT  In the beginning God created the heavens and t...\n",
      "1     OT  The earth was without form, and void; and dark...\n",
      "2     OT  Then God said, \"Let there be light\"; and there...\n",
      "3     OT  And God saw the light, that it was good; and G...\n",
      "4     OT  God called the light Day, and the darkness He ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\", header=None, names=[\"corpus\", \"text\"])\n",
    "print(\"Loaded data shape:\", df.shape)\n",
    "print(\"Example rows:\")\n",
    "print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc923b99-fbb9-4e03-9b4f-4b86f88b6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing, example cleaned verses:\n",
      "  corpus                                               text  \\\n",
      "0     OT  In the beginning God created the heavens and t...   \n",
      "1     OT  The earth was without form, and void; and dark...   \n",
      "2     OT  Then God said, \"Let there be light\"; and there...   \n",
      "3     OT  And God saw the light, that it was good; and G...   \n",
      "4     OT  God called the light Day, and the darkness He ...   \n",
      "\n",
      "                                               clean  \n",
      "0                       begin god creat heaven earth  \n",
      "1  earth form void dark face deep spirit god hove...  \n",
      "2                                    god light light  \n",
      "3                god light good god divid light dark  \n",
      "4   god call light day dark call night even morn day   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "  \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords and very short tokens, then stem\n",
    "    processed_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "        if len(tok) <= 1:\n",
    "            continue\n",
    "        stemmed = stemmer.stem(tok)\n",
    "        processed_tokens.append(stemmed)\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "print(\"After preprocessing, example cleaned verses:\")\n",
    "print(df[[\"corpus\", \"text\", \"clean\"]].head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc64519-e2d2-4a51-b4ee-a9d94ac3125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document–term matrix shape: (30367, 8765)\n",
      "Number of unique tokens: 8765 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean\"])\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Document–term matrix shape:\", X.shape)\n",
    "print(\"Number of unique tokens:\", len(feature_names), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53a0de6d-e3f6-42bb-ae67-1b13938b9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected corpus labels: ['NT', 'OT', 'Quran'] \n",
      "\n",
      "=== Computing MI and χ² for corpus: NT ===\n",
      "Saved MI scores to   mi_nt.csv\n",
      "Saved χ² scores to   chi2_nt.csv\n",
      "\n",
      "=== Computing MI and χ² for corpus: OT ===\n",
      "Saved MI scores to   mi_ot.csv\n",
      "Saved χ² scores to   chi2_ot.csv\n",
      "\n",
      "=== Computing MI and χ² for corpus: Quran ===\n",
      "Saved MI scores to   mi_quran.csv\n",
      "Saved χ² scores to   chi2_quran.csv\n",
      "\n",
      "MI/χ² computation finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_labels = sorted(df[\"corpus\"].unique())\n",
    "print(\"Detected corpus labels:\", corpus_labels, \"\\n\")\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    print(f\"=== Computing MI and χ² for corpus: {corpus_label} ===\")\n",
    "\n",
    "    # Binary target: this corpus vs all others\n",
    "    y_binary = (df[\"corpus\"] == corpus_label).astype(int).values\n",
    "\n",
    "    # Mutual Information \n",
    "    mi_scores = mutual_info_classif(\n",
    "        X, y_binary, discrete_features=True, random_state=0\n",
    "    )\n",
    "\n",
    "    mi_df = pd.DataFrame({\n",
    "        \"token\": feature_names,\n",
    "        \"score\": mi_scores\n",
    "    }).sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # Chi-square\n",
    "    chi2_scores, _ = chi2(X, y_binary)\n",
    "\n",
    "    chi2_df = pd.DataFrame({\n",
    "        \"token\": feature_names,\n",
    "        \"score\": chi2_scores\n",
    "    }).sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # Save to CSV as required: token,score\n",
    "    safe_label = corpus_label.lower().replace(\" \", \"_\")\n",
    "    mi_filename = f\"mi_{safe_label}.csv\"\n",
    "    chi2_filename = f\"chi2_{safe_label}.csv\"\n",
    "\n",
    "    mi_df.to_csv(mi_filename, index=False)\n",
    "    chi2_df.to_csv(chi2_filename, index=False)\n",
    "\n",
    "    print(f\"Saved MI scores to   {mi_filename}\")\n",
    "    print(f\"Saved χ² scores to   {chi2_filename}\\n\")\n",
    "\n",
    "print(\"MI/χ² computation finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d07ffae-3c22-4bdf-acdf-000b577a94cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA with 20 topics...\n",
      "LDA fitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fitting LDA with {N_TOPICS} topics...\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=N_TOPICS,\n",
    "    random_state=0,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "\n",
    "doc_topic_probs = lda.fit_transform(X)\n",
    "print(\"LDA fitted.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed996ab5-9f9b-45f7-90f5-0e7e75b2334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: NT\n",
      "  First few average topic scores: [0.0266 0.0483 0.0281 0.0309 0.0449]\n",
      "  Top topic index for this corpus: 5\n",
      "\n",
      "Corpus: OT\n",
      "  First few average topic scores: [0.0496 0.0392 0.0837 0.0561 0.0464]\n",
      "  Top topic index for this corpus: 19\n",
      "\n",
      "Corpus: Quran\n",
      "  First few average topic scores: [0.0216 0.0163 0.0207 0.0164 0.0695]\n",
      "  Top topic index for this corpus: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_avgs = {}\n",
    "top_topic_index = {}\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    mask = (df[\"corpus\"] == corpus_label).values\n",
    "    avg = doc_topic_probs[mask].mean(axis=0)  # average over docs\n",
    "    topic_avgs[corpus_label] = avg\n",
    "    best_topic = int(np.argmax(avg))\n",
    "    top_topic_index[corpus_label] = best_topic\n",
    "\n",
    "    print(f\"Corpus: {corpus_label}\")\n",
    "    print(\"  First few average topic scores:\", np.round(avg[:5], 4))\n",
    "    print(f\"  Top topic index for this corpus: {best_topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc025d0c-3849-419a-85d4-4613715e9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Corpus: NT — Dominant Topic 5 ===\n",
      "jesu             881.0184\n",
      "thing            811.7532\n",
      "god              605.4264\n",
      "christ           501.3980\n",
      "spirit           299.6900\n",
      "faith            265.0224\n",
      "work             236.4003\n",
      "man              187.6697\n",
      "discipl          180.6544\n",
      "receiv           171.1061\n",
      "\n",
      "=== Corpus: OT — Dominant Topic 19 ===\n",
      "lord             1799.4951\n",
      "god              880.4480\n",
      "word             807.6088\n",
      "hear             604.6053\n",
      "sin              576.0461\n",
      "command          563.7937\n",
      "nt               408.1103\n",
      "heart            400.7994\n",
      "law              394.7199\n",
      "israel           388.7234\n",
      "\n",
      "=== Corpus: Quran — Dominant Topic 8 ===\n",
      "god              3084.6533\n",
      "lord             915.4582\n",
      "peopl            523.8709\n",
      "fear             494.8636\n",
      "merci            453.4935\n",
      "messeng          444.2564\n",
      "worship          398.6101\n",
      "believ           397.0213\n",
      "forgiv           248.3855\n",
      "seek             243.0494\n",
      "\n",
      "LDA topic analysis complete.\n",
      "Top-topic tokens per corpus saved as lda_top_topic_tokens_<corpus>.csv\n"
     ]
    }
   ],
   "source": [
    "def top_tokens_for_topic(topic_idx: int, n: int = 10):\n",
    "    topic_vector = lda.components_[topic_idx]\n",
    "    top_ids = topic_vector.argsort()[::-1][:n]\n",
    "    tokens = feature_names[top_ids]\n",
    "    weights = topic_vector[top_ids]\n",
    "    return tokens, weights\n",
    "\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    t_idx = top_topic_index[corpus_label]\n",
    "    tokens, weights = top_tokens_for_topic(t_idx, n=10)\n",
    "\n",
    "    print(f\"=== Corpus: {corpus_label} — Dominant Topic {t_idx} ===\")\n",
    "    for tok, w in zip(tokens, weights):\n",
    "        print(f\"{tok:15s}  {w:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Save to CSV for report tables\n",
    "    safe_label = corpus_label.lower().replace(\" \", \"_\")\n",
    "    out_df = pd.DataFrame({\"token\": tokens, \"weight\": weights})\n",
    "    out_df.to_csv(f\"lda_top_topic_tokens_{safe_label}.csv\", index=False)\n",
    "\n",
    "print(\"LDA topic analysis complete.\")\n",
    "print(\"Top-topic tokens per corpus saved as lda_top_topic_tokens_<corpus>.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0279335-b455-4d13-8da2-de251b88007e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
