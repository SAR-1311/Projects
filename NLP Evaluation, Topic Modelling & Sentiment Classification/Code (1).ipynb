{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ac6402-8439-4bb4-9e6e-e07fe560d07d",
   "metadata": {},
   "source": [
    "### Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c6d4e7-58e9-4182-839d-7e6b1662e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d23e5bc-4a68-48c4-a5de-92c5206e6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SYSTEM_RESULTS = \"system_results.csv\"   \n",
    "INPUT_QRELS = \"qrels.csv\"\n",
    "OUTPUT_EVAL = \"ir_eval.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39c7ec3-4d6f-4d3b-8d07-625983a933bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ranked_docs, rel_set, k):\n",
    "    if k <= 0:\n",
    "        return 0.0\n",
    "    top_docs = ranked_docs[:k]\n",
    "    if not top_docs:\n",
    "        return 0.0\n",
    "    rel_count = sum(1 for d in top_docs if d in rel_set)\n",
    "    return rel_count / len(top_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696b1b81-e9fc-4380-8de9-ce8a8719da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(ranked_docs, rel_set, k):\n",
    "    if not rel_set:\n",
    "        return 0.0\n",
    "    top_docs = ranked_docs[:k]\n",
    "    rel_count = sum(1 for d in top_docs if d in rel_set)\n",
    "    return rel_count / len(rel_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ff8545-0ac3-4fbe-8ca0-04ed33612260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(ranked_docs, rel_set):\n",
    "    R = len(rel_set)\n",
    "    if R == 0:\n",
    "        return 0.0\n",
    "    return precision_at_k(ranked_docs, rel_set, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14a438c-a608-4ad3-835b-ec200b9770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(ranked_docs, rel_set):\n",
    "    R = len(rel_set)\n",
    "    if R == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sum_prec = 0.0\n",
    "    rel_seen = 0\n",
    "    for i, doc in enumerate(ranked_docs, start=1):\n",
    "        if doc in rel_set:\n",
    "            rel_seen += 1\n",
    "            sum_prec += rel_seen / i\n",
    "    return sum_prec / R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a091b954-7c08-4944-90c3-9bd8624e7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(ranked_docs, rel_grades, k):\n",
    "    dcg = 0.0\n",
    "    for i, doc in enumerate(ranked_docs[:k], start=1):\n",
    "        rel = rel_grades.get(doc, 0)\n",
    "        if i == 1:\n",
    "            dcg += rel\n",
    "        else:\n",
    "            dcg += rel / log2(i)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c5cf63-ce84-40e6-85fc-a8e9e88bb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(ranked_docs, rel_grades, k):\n",
    "    if not rel_grades:\n",
    "        return 0.0\n",
    "\n",
    "    dcg = dcg_at_k(ranked_docs, rel_grades, k)\n",
    "    ideal_docs_sorted = sorted(rel_grades.items(), key=lambda x: x[1], reverse=True)\n",
    "    ideal_ranked_docs = [doc_id for doc_id, _ in ideal_docs_sorted]\n",
    "    ideal_dcg = dcg_at_k(ideal_ranked_docs, rel_grades, k)\n",
    "\n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937ab9d7-641a-47ba-98d4-b71f9253e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation results to ir_eval.csv\n"
     ]
    }
   ],
   "source": [
    "def evaluate_systems(system_results_path, qrels_path, output_path):\n",
    "    # Load the input CSVs\n",
    "    system_results = pd.read_csv('ttdssystemresults.csv')\n",
    "    qrels = pd.read_csv('qrels.csv')\n",
    "\n",
    "    # Binary relevance: rel>0 -> relevant\n",
    "    qrels_binary = {\n",
    "        q: set(sub.loc[sub[\"relevance\"] > 0, \"doc_id\"])\n",
    "        for q, sub in qrels.groupby(\"query_id\")\n",
    "    }\n",
    "\n",
    "    # Graded relevance\n",
    "    qrels_graded = {\n",
    "        q: dict(zip(sub[\"doc_id\"], sub[\"relevance\"]))\n",
    "        for q, sub in qrels.groupby(\"query_id\")\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Loop over systems \n",
    "    for system_id, sys_group in system_results.groupby(\"system_number\"):\n",
    "        per_query_metrics = {}\n",
    "\n",
    "        # Loop over queries \n",
    "        for query_id, q_group in sys_group.groupby(\"query_number\"):\n",
    "            ranked_docs = (\n",
    "                q_group.sort_values(\"rank_of_doc\")[\"doc_number\"].tolist()\n",
    "            )\n",
    "\n",
    "            rel_set = qrels_binary.get(query_id, set())\n",
    "            rel_grades = qrels_graded.get(query_id, {})\n",
    "\n",
    "            P10 = precision_at_k(ranked_docs, rel_set, 10)\n",
    "            R50 = recall_at_k(ranked_docs, rel_set, 50)\n",
    "            Rprec = r_precision(ranked_docs, rel_set)\n",
    "            AP_val = average_precision(ranked_docs, rel_set)\n",
    "            nDCG10 = ndcg_at_k(ranked_docs, rel_grades, 10)\n",
    "            nDCG20 = ndcg_at_k(ranked_docs, rel_grades, 20)\n",
    "\n",
    "            per_query_metrics[query_id] = (\n",
    "                P10, R50, Rprec, AP_val, nDCG10, nDCG20\n",
    "            )\n",
    "\n",
    "            # One row per (system, query)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"system_number\": system_id,\n",
    "                    \"query_number\": query_id,\n",
    "                    \"P@10\": P10,\n",
    "                    \"R@50\": R50,\n",
    "                    \"r-precision\": Rprec,\n",
    "                    \"AP\": AP_val,\n",
    "                    \"nDCG@10\": nDCG10,\n",
    "                    \"nDCG@20\": nDCG20,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Mean row per system across the 10 queries\n",
    "        if per_query_metrics:\n",
    "            metrics_array = np.array(list(per_query_metrics.values()))  # shape (10,6)\n",
    "            means = metrics_array.mean(axis=0)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"system_number\": system_id,\n",
    "                    \"query_number\": \"mean\",\n",
    "                    \"P@10\": means[0],\n",
    "                    \"R@50\": means[1],\n",
    "                    \"r-precision\": means[2],\n",
    "                    \"AP\": means[3],\n",
    "                    \"nDCG@10\": means[4],\n",
    "                    \"nDCG@20\": means[5],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Build DataFrame in the EXACT column order required\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df = out_df[\n",
    "        [\n",
    "            \"system_number\",\n",
    "            \"query_number\",\n",
    "            \"P@10\",\n",
    "            \"R@50\",\n",
    "            \"r-precision\",\n",
    "            \"AP\",\n",
    "            \"nDCG@10\",\n",
    "            \"nDCG@20\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Sort: for each system_number, queries 1..10 then \"mean\"\n",
    "    def query_sort_key(q):\n",
    "        if isinstance(q, str):\n",
    "            return 9999  \n",
    "        return int(q)\n",
    "\n",
    "    out_df[\"query_sort\"] = out_df[\"query_number\"].apply(query_sort_key)\n",
    "    out_df = out_df.sort_values([\"system_number\", \"query_sort\"]).drop(columns=[\"query_sort\"])\n",
    "\n",
    "    # Write CSV with values rounded to 3 decimal places\n",
    "    out_df.to_csv(output_path, index=False, float_format=\"%.3f\")\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_eval = evaluate_systems(INPUT_SYSTEM_RESULTS, INPUT_QRELS, OUTPUT_EVAL)\n",
    "    print(f\"Saved evaluation results to {OUTPUT_EVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c1d01f-95f3-424e-a0eb-71d3176d18a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation results to ir_eval.csv\n",
      "    system_number query_number  P@10      R@50  r-precision        AP  \\\n",
      "0               1            1  0.40  0.666667     0.166667  0.244022   \n",
      "1               1            2  0.30  1.000000     0.250000  0.405211   \n",
      "2               1            3  0.00  1.000000     0.000000  0.050000   \n",
      "3               1            4  0.60  0.875000     0.700000  0.625316   \n",
      "4               1            5  0.20  0.428571     0.285714  0.119048   \n",
      "5               1            6  0.70  1.000000     0.750000  0.692321   \n",
      "6               1            7  0.20  0.666667     0.333333  0.233333   \n",
      "7               1            8  0.60  1.000000     0.625000  0.704088   \n",
      "8               1            9  0.90  0.900000     0.900000  0.850429   \n",
      "9               1           10  0.00  0.800000     0.000000  0.078333   \n",
      "10              1         mean  0.39  0.833690     0.401071  0.400210   \n",
      "11              2            1  0.10  0.666667     0.000000  0.088808   \n",
      "\n",
      "     nDCG@10   nDCG@20  \n",
      "0   0.395434  0.395434  \n",
      "1   0.238234  0.530351  \n",
      "2   0.000000  0.231378  \n",
      "3   0.453254  0.577326  \n",
      "4   0.236437  0.292147  \n",
      "5   0.618879  0.750640  \n",
      "6   0.586232  0.586232  \n",
      "7   0.616095  0.712073  \n",
      "8   0.484884  0.611824  \n",
      "9   0.000000  0.164364  \n",
      "10  0.362945  0.485177  \n",
      "11  0.087958  0.161563  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_eval = evaluate_systems(INPUT_SYSTEM_RESULTS, INPUT_QRELS, OUTPUT_EVAL)\n",
    "    print(f\"Saved evaluation results to {OUTPUT_EVAL}\")\n",
    "    print(df_eval.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d514a7a-a7d2-445f-8b4d-1118611f33e7",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e012c20f-3a40-41b9-b594-7a7e55e1ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19338105-9fef-4eb9-834c-8e80e6ea145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"bible_and_quran.tsv\"   \n",
    "STOPWORDS_PATH = \"stopwords.txt\"    \n",
    "N_TOPICS = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674a8372-2a0f-4c11-b2b5-1837dd726ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 570 custom stopwords from stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords(path: str) -> set:\n",
    "    stopwords_set = set()\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            w = line.strip().lower()\n",
    "            if w:\n",
    "                stopwords_set.add(w)\n",
    "    return stopwords_set\n",
    "\n",
    "\n",
    "stop_words = load_stopwords(STOPWORDS_PATH)\n",
    "print(f\"Loaded {len(stop_words)} custom stopwords from {STOPWORDS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3378a97b-6c0a-4a22-9d38-c58faab86640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (30367, 2)\n",
      "Example rows:\n",
      "  corpus                                               text\n",
      "0     OT  In the beginning God created the heavens and t...\n",
      "1     OT  The earth was without form, and void; and dark...\n",
      "2     OT  Then God said, \"Let there be light\"; and there...\n",
      "3     OT  And God saw the light, that it was good; and G...\n",
      "4     OT  God called the light Day, and the darkness He ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\", header=None, names=[\"corpus\", \"text\"])\n",
    "print(\"Loaded data shape:\", df.shape)\n",
    "print(\"Example rows:\")\n",
    "print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "083dcdb0-b31a-4790-ab48-f84ac7177e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing, example cleaned verses:\n",
      "  corpus                                               text  \\\n",
      "0     OT  In the beginning God created the heavens and t...   \n",
      "1     OT  The earth was without form, and void; and dark...   \n",
      "2     OT  Then God said, \"Let there be light\"; and there...   \n",
      "3     OT  And God saw the light, that it was good; and G...   \n",
      "4     OT  God called the light Day, and the darkness He ...   \n",
      "\n",
      "                                               clean  \n",
      "0                       begin god creat heaven earth  \n",
      "1  earth form void dark face deep spirit god hove...  \n",
      "2                                    god light light  \n",
      "3                god light good god divid light dark  \n",
      "4   god call light day dark call night even morn day   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "  \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords and very short tokens, then stem\n",
    "    processed_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "        if len(tok) <= 1:\n",
    "            continue\n",
    "        stemmed = stemmer.stem(tok)\n",
    "        processed_tokens.append(stemmed)\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "print(\"After preprocessing, example cleaned verses:\")\n",
    "print(df[[\"corpus\", \"text\", \"clean\"]].head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9277955c-d852-4562-bc72-73c9208018de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document–term matrix shape: (30367, 8765)\n",
      "Number of unique tokens: 8765 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean\"])\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Document–term matrix shape:\", X.shape)\n",
    "print(\"Number of unique tokens:\", len(feature_names), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27cf585e-4a3c-44e9-8332-8ff7b14170c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected corpus labels: ['NT', 'OT', 'Quran'] \n",
      "\n",
      "=== Computing MI and χ² for corpus: NT ===\n",
      "Saved MI scores to   mi_nt.csv\n",
      "Saved χ² scores to   chi2_nt.csv\n",
      "\n",
      "=== Computing MI and χ² for corpus: OT ===\n",
      "Saved MI scores to   mi_ot.csv\n",
      "Saved χ² scores to   chi2_ot.csv\n",
      "\n",
      "=== Computing MI and χ² for corpus: Quran ===\n",
      "Saved MI scores to   mi_quran.csv\n",
      "Saved χ² scores to   chi2_quran.csv\n",
      "\n",
      "MI/χ² computation finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_labels = sorted(df[\"corpus\"].unique())\n",
    "print(\"Detected corpus labels:\", corpus_labels, \"\\n\")\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    print(f\"=== Computing MI and χ² for corpus: {corpus_label} ===\")\n",
    "\n",
    "    # Binary target: this corpus vs all others\n",
    "    y_binary = (df[\"corpus\"] == corpus_label).astype(int).values\n",
    "\n",
    "    # Mutual Information \n",
    "    mi_scores = mutual_info_classif(\n",
    "        X, y_binary, discrete_features=True, random_state=0\n",
    "    )\n",
    "\n",
    "    mi_df = pd.DataFrame({\n",
    "        \"token\": feature_names,\n",
    "        \"score\": mi_scores\n",
    "    }).sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # Chi-square\n",
    "    chi2_scores, _ = chi2(X, y_binary)\n",
    "\n",
    "    chi2_df = pd.DataFrame({\n",
    "        \"token\": feature_names,\n",
    "        \"score\": chi2_scores\n",
    "    }).sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # Save to CSV as required: token,score\n",
    "    safe_label = corpus_label.lower().replace(\" \", \"_\")\n",
    "    mi_filename = f\"mi_{safe_label}.csv\"\n",
    "    chi2_filename = f\"chi2_{safe_label}.csv\"\n",
    "\n",
    "    mi_df.to_csv(mi_filename, index=False)\n",
    "    chi2_df.to_csv(chi2_filename, index=False)\n",
    "\n",
    "    print(f\"Saved MI scores to   {mi_filename}\")\n",
    "    print(f\"Saved χ² scores to   {chi2_filename}\\n\")\n",
    "\n",
    "print(\"MI/χ² computation finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc6dbc6-fe1e-415f-b1bc-7213e42fdb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA with 20 topics...\n",
      "LDA fitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fitting LDA with {N_TOPICS} topics...\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=N_TOPICS,\n",
    "    random_state=0,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "\n",
    "doc_topic_probs = lda.fit_transform(X)\n",
    "print(\"LDA fitted.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9830d328-a7d1-47f8-a496-70bf19ef6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: NT\n",
      "  First few average topic scores: [0.0266 0.0483 0.0281 0.0309 0.0449]\n",
      "  Top topic index for this corpus: 5\n",
      "\n",
      "Corpus: OT\n",
      "  First few average topic scores: [0.0496 0.0392 0.0837 0.0561 0.0464]\n",
      "  Top topic index for this corpus: 19\n",
      "\n",
      "Corpus: Quran\n",
      "  First few average topic scores: [0.0216 0.0163 0.0207 0.0164 0.0695]\n",
      "  Top topic index for this corpus: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_avgs = {}\n",
    "top_topic_index = {}\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    mask = (df[\"corpus\"] == corpus_label).values\n",
    "    avg = doc_topic_probs[mask].mean(axis=0)  # average over docs\n",
    "    topic_avgs[corpus_label] = avg\n",
    "    best_topic = int(np.argmax(avg))\n",
    "    top_topic_index[corpus_label] = best_topic\n",
    "\n",
    "    print(f\"Corpus: {corpus_label}\")\n",
    "    print(\"  First few average topic scores:\", np.round(avg[:5], 4))\n",
    "    print(f\"  Top topic index for this corpus: {best_topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c420d6-70fa-4748-abe5-b5ce8f38e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Corpus: NT — Dominant Topic 5 ===\n",
      "jesu             881.0184\n",
      "thing            811.7532\n",
      "god              605.4264\n",
      "christ           501.3980\n",
      "spirit           299.6900\n",
      "faith            265.0224\n",
      "work             236.4003\n",
      "man              187.6697\n",
      "discipl          180.6544\n",
      "receiv           171.1061\n",
      "\n",
      "=== Corpus: OT — Dominant Topic 19 ===\n",
      "lord             1799.4951\n",
      "god              880.4480\n",
      "word             807.6088\n",
      "hear             604.6053\n",
      "sin              576.0461\n",
      "command          563.7937\n",
      "nt               408.1103\n",
      "heart            400.7994\n",
      "law              394.7199\n",
      "israel           388.7234\n",
      "\n",
      "=== Corpus: Quran — Dominant Topic 8 ===\n",
      "god              3084.6533\n",
      "lord             915.4582\n",
      "peopl            523.8709\n",
      "fear             494.8636\n",
      "merci            453.4935\n",
      "messeng          444.2564\n",
      "worship          398.6101\n",
      "believ           397.0213\n",
      "forgiv           248.3855\n",
      "seek             243.0494\n",
      "\n",
      "LDA topic analysis complete.\n",
      "Top-topic tokens per corpus saved as lda_top_topic_tokens_<corpus>.csv\n"
     ]
    }
   ],
   "source": [
    "def top_tokens_for_topic(topic_idx: int, n: int = 10):\n",
    "    topic_vector = lda.components_[topic_idx]\n",
    "    top_ids = topic_vector.argsort()[::-1][:n]\n",
    "    tokens = feature_names[top_ids]\n",
    "    weights = topic_vector[top_ids]\n",
    "    return tokens, weights\n",
    "\n",
    "\n",
    "for corpus_label in corpus_labels:\n",
    "    t_idx = top_topic_index[corpus_label]\n",
    "    tokens, weights = top_tokens_for_topic(t_idx, n=10)\n",
    "\n",
    "    print(f\"=== Corpus: {corpus_label} — Dominant Topic {t_idx} ===\")\n",
    "    for tok, w in zip(tokens, weights):\n",
    "        print(f\"{tok:15s}  {w:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Save to CSV for report tables\n",
    "    safe_label = corpus_label.lower().replace(\" \", \"_\")\n",
    "    out_df = pd.DataFrame({\"token\": tokens, \"weight\": weights})\n",
    "    out_df.to_csv(f\"lda_top_topic_tokens_{safe_label}.csv\", index=False)\n",
    "\n",
    "print(\"LDA topic analysis complete.\")\n",
    "print(\"Top-topic tokens per corpus saved as lda_top_topic_tokens_<corpus>.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60802ec9-554f-40cf-9815-7241db3340dc",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "940235d8-8593-4578-97dd-19c3fb2c9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b45d3663-d398-4979-84a8-5ced94749516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7546cfb-33b0-4c4a-b57c-6c2a1a9b7717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/dev data: (18646, 3)\n",
      "sentiment\n",
      "neutral     8789\n",
      "positive    5979\n",
      "negative    3878\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Loaded test data: (4662, 3)\n",
      "sentiment\n",
      "neutral     2197\n",
      "positive    1495\n",
      "negative     970\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"train.txt\"                \n",
    "test_path  = \"ttds_2025_cw2_test.txt\"   \n",
    "\n",
    "# train/dev data\n",
    "train_df = pd.read_csv(\n",
    "    train_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"id\", \"sentiment\", \"tweet\"],\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# keep only the 3 sentiment labels\n",
    "train_df = train_df[train_df[\"sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "\n",
    "print(\"Loaded train/dev data:\", train_df.shape)\n",
    "print(train_df[\"sentiment\"].value_counts(), \"\\n\")\n",
    "\n",
    "# labelled test data\n",
    "test_df = pd.read_csv(\n",
    "    test_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"id\", \"sentiment\", \"tweet\"],\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "test_df = test_df[test_df[\"sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "\n",
    "print(\"Loaded test data:\", test_df.shape)\n",
    "print(test_df[\"sentiment\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cc48b89-d713-4a92-9f68-9f8015367df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 16781\n",
      "Dev split size:   1865 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_split_df, dev_split_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,             # 90% train, 10% dev\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"sentiment\"]\n",
    ")\n",
    "\n",
    "print(\"Train split size:\", len(train_split_df))\n",
    "print(\"Dev split size:  \", len(dev_split_df), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d604db-280c-4e64-80de-f4d1805a8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_re = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    text = str(text)\n",
    "    text = punct_re.sub(\" \", text)\n",
    "    text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "# tokenised docs\n",
    "train_tokens = [tokenize(t) for t in train_split_df[\"tweet\"]]\n",
    "dev_tokens   = [tokenize(t) for t in dev_split_df[\"tweet\"]]\n",
    "test_tokens  = [tokenize(t) for t in test_df[\"tweet\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a91847-c1f8-4009-847c-c3d9f59ae074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37127\n",
      "Feature matrix shapes:\n",
      "  X_train: (16781, 37128)\n",
      "  X_dev:   (1865, 37128)\n",
      "  X_test:  (4662, 37128) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(docs_tokens):\n",
    "    vocab = set()\n",
    "    for doc in docs_tokens:\n",
    "        vocab.update(doc)\n",
    "    return {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "\n",
    "word2id = build_vocab(train_tokens)\n",
    "vocab_size = len(word2id)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "def convert_to_bow_matrix(preprocessed_docs, word2id):\n",
    "    n_docs = len(preprocessed_docs)\n",
    "    oov_index = len(word2id)\n",
    "    mat_size = (n_docs, oov_index + 1)\n",
    "\n",
    "    X_dok = scipy.sparse.dok_matrix(mat_size, dtype=np.int32)\n",
    "\n",
    "    for doc_id, doc in enumerate(preprocessed_docs):\n",
    "        for word in doc:\n",
    "            token_id = word2id.get(word, oov_index)\n",
    "            X_dok[doc_id, token_id] += 1\n",
    "\n",
    "    return X_dok.tocsr()\n",
    "\n",
    "X_train = convert_to_bow_matrix(train_tokens, word2id)\n",
    "X_dev   = convert_to_bow_matrix(dev_tokens,   word2id)\n",
    "X_test  = convert_to_bow_matrix(test_tokens,  word2id)\n",
    "\n",
    "print(\"Feature matrix shapes:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_dev:  \", X_dev.shape)\n",
    "print(\"  X_test: \", X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ea0add2-ee12-4c83-a81f-4a3e0cf7c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment2id = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "id2sentiment = {v: k for k, v in sentiment2id.items()}\n",
    "\n",
    "y_train = train_split_df[\"sentiment\"].map(sentiment2id).to_numpy()\n",
    "y_dev   = dev_split_df[\"sentiment\"].map(sentiment2id).to_numpy()\n",
    "y_test  = test_df[\"sentiment\"].map(sentiment2id).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b87aa84-6279-4293-af48-7552b3cdf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred):\n",
    "    labels = [\n",
    "        sentiment2id[\"positive\"],\n",
    "        sentiment2id[\"negative\"],\n",
    "        sentiment2id[\"neutral\"],\n",
    "    ]\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    scores = {\n",
    "        \"p-pos\": p[0], \"r-pos\": r[0], \"f-pos\": f[0],\n",
    "        \"p-neg\": p[1], \"r-neg\": r[1], \"f-neg\": f[1],\n",
    "        \"p-neu\": p[2], \"r-neu\": r[2], \"f-neu\": f[2],\n",
    "        \"p-macro\": p.mean(),\n",
    "        \"r-macro\": r.mean(),\n",
    "        \"f-macro\": f.mean(),\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c86729a-80e0-4c8a-9523-d1af8a3dd81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " BASELINE MODEL: SVC (BOW)     \n",
      "================================\n",
      "Baseline dev macro F1: 0.5603\n",
      "Baseline test macro F1: 0.5583 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================================\")\n",
    "print(\" BASELINE MODEL: SVC (BOW)     \")\n",
    "print(\"================================\")\n",
    "\n",
    "baseline_clf = SVC(C=1000, kernel=\"linear\")\n",
    "\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_baseline = baseline_clf.predict(X_train)\n",
    "y_dev_pred_baseline   = baseline_clf.predict(X_dev)\n",
    "y_test_pred_baseline  = baseline_clf.predict(X_test)\n",
    "\n",
    "baseline_dev_scores = compute_scores(y_dev, y_dev_pred_baseline)\n",
    "baseline_test_scores = compute_scores(y_test, y_test_pred_baseline)\n",
    "\n",
    "print(\"Baseline dev macro F1:\",\n",
    "      f\"{baseline_dev_scores['f-macro']:.4f}\")\n",
    "print(\"Baseline test macro F1:\",\n",
    "      f\"{baseline_test_scores['f-macro']:.4f}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af195ec3-444a-412e-a18f-79abe1573ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- LogisticRegression (BOW) -----\n",
      "Dev macro F1:  0.6147\n",
      "Test macro F1: 0.6094 \n",
      "\n",
      "----- RandomForestClassifier (BOW) -----\n",
      "Dev macro F1:  0.4843\n",
      "Test macro F1: 0.4783 \n",
      "\n",
      "----- MultinomialNB (BOW) -----\n",
      "Dev macro F1:  0.5926\n",
      "Test macro F1: 0.5986 \n",
      "\n",
      "----- LinearSVC (BOW) -----\n",
      "Dev macro F1:  0.5957\n",
      "Test macro F1: 0.5810 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_eval_model(name, clf, X_train, y_train, X_dev, y_dev, X_test, y_test,\n",
    "                         use_dense=False):\n",
    "    \n",
    "    if use_dense:\n",
    "        X_train_fit = X_train.toarray()\n",
    "        X_dev_fit   = X_dev.toarray()\n",
    "        X_test_fit  = X_test.toarray()\n",
    "    else:\n",
    "        X_train_fit = X_train\n",
    "        X_dev_fit   = X_dev\n",
    "        X_test_fit  = X_test\n",
    "\n",
    "    print(f\"----- {name} -----\")\n",
    "    clf.fit(X_train_fit, y_train)\n",
    "\n",
    "    y_train_pred = clf.predict(X_train_fit)\n",
    "    y_dev_pred   = clf.predict(X_dev_fit)\n",
    "    y_test_pred  = clf.predict(X_test_fit)\n",
    "\n",
    "    dev_scores = compute_scores(y_dev, y_dev_pred)\n",
    "    test_scores = compute_scores(y_test, y_test_pred)\n",
    "\n",
    "    print(\"Dev macro F1: \", f\"{dev_scores['f-macro']:.4f}\")\n",
    "    print(\"Test macro F1:\", f\"{test_scores['f-macro']:.4f}\", \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"clf\": clf,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"y_dev_pred\": y_dev_pred,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"dev_scores\": dev_scores,\n",
    "        \"test_scores\": test_scores,\n",
    "    }\n",
    "\n",
    "improved_candidates = []\n",
    "\n",
    "# 1) Logistic Regression \n",
    "logreg = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
    "improved_candidates.append(\n",
    "    train_and_eval_model(\n",
    "        \"LogisticRegression (BOW)\",\n",
    "        logreg,\n",
    "        X_train, y_train,\n",
    "        X_dev,   y_dev,\n",
    "        X_test,  y_test,\n",
    "        use_dense=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Random Forest \n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "improved_candidates.append(\n",
    "    train_and_eval_model(\n",
    "        \"RandomForestClassifier (BOW)\",\n",
    "        rf,\n",
    "        X_train, y_train,\n",
    "        X_dev,   y_dev,\n",
    "        X_test,  y_test,\n",
    "        use_dense=True  \n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) Multinomial Naive Bayes \n",
    "mnb = MultinomialNB()\n",
    "improved_candidates.append(\n",
    "    train_and_eval_model(\n",
    "        \"MultinomialNB (BOW)\",\n",
    "        mnb,\n",
    "        X_train, y_train,\n",
    "        X_dev,   y_dev,\n",
    "        X_test,  y_test,\n",
    "        use_dense=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) LinearSVC \n",
    "linsvm = LinearSVC(C=1.0)\n",
    "improved_candidates.append(\n",
    "    train_and_eval_model(\n",
    "        \"LinearSVC (BOW)\",\n",
    "        linsvm,\n",
    "        X_train, y_train,\n",
    "        X_dev,   y_dev,\n",
    "        X_test,  y_test,\n",
    "        use_dense=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6414830-e7ef-4357-ac93-60f7774572cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      " BEST IMPROVED MODEL (among 4 candidates)\n",
      " Model:        LogisticRegression (BOW)\n",
      " Dev macro F1: 0.6147\n",
      " Test macro F1: 0.6094\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_candidate = None\n",
    "best_f_macro = -1.0\n",
    "\n",
    "for cand in improved_candidates:\n",
    "    f_macro = cand[\"dev_scores\"][\"f-macro\"]\n",
    "    if f_macro > best_f_macro:\n",
    "        best_f_macro = f_macro\n",
    "        best_candidate = cand\n",
    "\n",
    "print(\"=============================================\")\n",
    "print(\" BEST IMPROVED MODEL (among 4 candidates)\")\n",
    "print(\" Model:       \", best_candidate[\"name\"])\n",
    "print(\" Dev macro F1:\", f\"{best_candidate['dev_scores']['f-macro']:.4f}\")\n",
    "print(\" Test macro F1:\",\n",
    "      f\"{best_candidate['test_scores']['f-macro']:.4f}\")\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "improved_name = best_candidate[\"name\"]\n",
    "improved_clf = best_candidate[\"clf\"]\n",
    "y_train_pred_improved = best_candidate[\"y_train_pred\"]\n",
    "y_dev_pred_improved   = best_candidate[\"y_dev_pred\"]\n",
    "y_test_pred_improved  = best_candidate[\"y_test_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f2aee93-03bc-4b05-8cd3-26b5db3e8055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classification.csv with rows:\n",
      "baseline train  f-macro=0.9993\n",
      "baseline dev    f-macro=0.5603\n",
      "baseline test   f-macro=0.5583\n",
      "improved train  f-macro=0.9721\n",
      "improved dev    f-macro=0.6147\n",
      "improved test   f-macro=0.6094\n"
     ]
    }
   ],
   "source": [
    "def add_result_row(result_list, system_name, split_name, y_true, y_pred):\n",
    "    scores = compute_scores(y_true, y_pred)\n",
    "    row = {\"system\": system_name, \"split\": split_name}\n",
    "    row.update(scores)\n",
    "    result_list.append(row)\n",
    "\n",
    "results = []\n",
    "\n",
    "# baseline rows\n",
    "add_result_row(results, \"baseline\", \"train\", y_train, y_train_pred_baseline)\n",
    "add_result_row(results, \"baseline\", \"dev\",   y_dev,   y_dev_pred_baseline)\n",
    "add_result_row(results, \"baseline\", \"test\",  y_test,  y_test_pred_baseline)\n",
    "\n",
    "# improved rows (using best candidate)\n",
    "add_result_row(results, \"improved\", \"train\", y_train, y_train_pred_improved)\n",
    "add_result_row(results, \"improved\", \"dev\",   y_dev,   y_dev_pred_improved)\n",
    "add_result_row(results, \"improved\", \"test\",  y_test,  y_test_pred_improved)\n",
    "\n",
    "header = [\n",
    "    \"system\", \"split\",\n",
    "    \"p-pos\", \"r-pos\", \"f-pos\",\n",
    "    \"p-neg\", \"r-neg\", \"f-neg\",\n",
    "    \"p-neu\", \"r-neu\", \"f-neu\",\n",
    "    \"p-macro\", \"r-macro\", \"f-macro\"\n",
    "]\n",
    "\n",
    "with open(\"classification.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Saved classification.csv with rows:\")\n",
    "for row in results:\n",
    "    print(f\"{row['system']:8s} {row['split']:5s}  f-macro={row['f-macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072a7dc-a566-40e5-9002-238a5ac07732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
