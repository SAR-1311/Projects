{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79c84f1e-f7ea-4545-a254-7c758355a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "340f48fc-b005-4058-b5da-30e27b0f431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id sentiment  \\\n",
      "0  638165350966669312  negative   \n",
      "1  640169120600862720   neutral   \n",
      "2  635946254254624769   neutral   \n",
      "3  667121920333258752  negative   \n",
      "4  628637519643586560   neutral   \n",
      "\n",
      "                                               tweet  \n",
      "0  Nicki's butt is just too big like c'mon that's...  \n",
      "1  Haruna Lukmon may av just played himself out o...  \n",
      "2  Zach Putnam will be unavailable for the White ...  \n",
      "3  \"\"\"@daithimckay what about the victims of IRA ...  \n",
      "4  \"\"\"LHP Matt Boyd, traded to @tigers in David P...  \n",
      "sentiment\n",
      "neutral     8789\n",
      "positive    5979\n",
      "negative    3878\n",
      "Name: count, dtype: int64\n",
      "Train size: 16781\n",
      "Dev size: 1865\n",
      "sentiment\n",
      "neutral     7910\n",
      "positive    5381\n",
      "negative    3490\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "neutral     879\n",
      "positive    598\n",
      "negative    388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---------- LOAD LABELED DATA ----------\n",
    "train_path = \"train.txt\"  # your provided file\n",
    "df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "\n",
    "# make sure we only keep the three labels we care about\n",
    "df = df[df[\"sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"sentiment\"].value_counts())\n",
    "\n",
    "# ---------- SHUFFLE + SPLIT INTO TRAIN / DEV ----------\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,              # you can change this ratio\n",
    "    random_state=42,\n",
    "    stratify=df[\"sentiment\"]    # keep label proportions\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Dev size:\", len(dev_df))\n",
    "print(train_df[\"sentiment\"].value_counts())\n",
    "print(dev_df[\"sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bbc4bdd-32a5-4800-8a88-6ecbd3555600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 37127\n"
     ]
    }
   ],
   "source": [
    "# basic tokeniser: lowercase, remove punctuation, split on whitespace\n",
    "punct_re = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "\n",
    "def tokenize(text):\n",
    "    text = punct_re.sub(\" \", str(text))\n",
    "    text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "# tokenise train and dev\n",
    "train_tokens = [tokenize(t) for t in train_df[\"tweet\"]]\n",
    "dev_tokens   = [tokenize(t) for t in dev_df[\"tweet\"]]\n",
    "\n",
    "# build vocabulary ONLY from the training split\n",
    "def build_vocab(docs_tokens):\n",
    "    vocab = set()\n",
    "    for doc in docs_tokens:\n",
    "        vocab.update(doc)\n",
    "    # give each word a unique ID\n",
    "    return {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "\n",
    "word2id = build_vocab(train_tokens)\n",
    "vocab_size = len(word2id)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# map labels to IDs in the order pos, neg, neu\n",
    "sentiment2id = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "id2sentiment = {v: k for k, v in sentiment2id.items()}\n",
    "\n",
    "y_train = train_df[\"sentiment\"].map(sentiment2id).to_numpy()\n",
    "y_dev   = dev_df[\"sentiment\"].map(sentiment2id).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbedf110-136b-4687-b52a-79165e0497cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline feature shapes: (16781, 37128) (1865, 37128)\n"
     ]
    }
   ],
   "source": [
    "def convert_to_bow_matrix(preprocessed_docs, word2id):\n",
    "    \"\"\"\n",
    "    preprocessed_docs: list of token lists\n",
    "    word2id: dict mapping word -> column index\n",
    "    \n",
    "    Returns: csr_matrix [n_docs x (vocab_size + 1)]\n",
    "             last column is OOV bucket\n",
    "    \"\"\"\n",
    "    n_docs = len(preprocessed_docs)\n",
    "    oov_index = len(word2id)\n",
    "    mat_size = (n_docs, oov_index + 1)\n",
    "    \n",
    "    X_dok = scipy.sparse.dok_matrix(mat_size, dtype=np.int32)\n",
    "    \n",
    "    for doc_id, doc in enumerate(preprocessed_docs):\n",
    "        for word in doc:\n",
    "            token_id = word2id.get(word, oov_index)  # OOV -> last column\n",
    "            X_dok[doc_id, token_id] += 1\n",
    "    \n",
    "    # convert to CSR for faster training\n",
    "    return X_dok.tocsr()\n",
    "\n",
    "X_train_baseline = convert_to_bow_matrix(train_tokens, word2id)\n",
    "X_dev_baseline   = convert_to_bow_matrix(dev_tokens, word2id)\n",
    "\n",
    "print(\"Baseline feature shapes:\", X_train_baseline.shape, X_dev_baseline.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41d35233-4c55-4713-98db-d385b4d18cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- BASELINE MODEL ----------\n",
    "baseline_clf = SVC(C=1000, kernel=\"linear\")  # or LinearSVC(C=1000)\n",
    "\n",
    "baseline_clf.fit(X_train_baseline, y_train)\n",
    "\n",
    "y_train_pred_baseline = baseline_clf.predict(X_train_baseline)\n",
    "y_dev_pred_baseline   = baseline_clf.predict(X_dev_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c5cb176-5fc5-411b-a0fa-07fc6a839387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 misclassified dev examples (baseline):\n",
      "\n",
      "Tweet ID: 624660020832002048\n",
      "Tweet: Can't wait for @Korn @suicidesilence in October. Hopefully I run into @DanKenny so I can bust his balls about Conor McGregor.\n",
      "Gold: neutral\n",
      "Pred: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Tweet ID: 636914085506904065\n",
      "Tweet: Why is Rousey worried bout money may pay and I'm pretty sure they not even in the same tax bracket\n",
      "Gold: neutral\n",
      "Pred: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Tweet ID: 639528821956460548\n",
      "Tweet: I'm selling a lawn ticket for Jason Aldean this coming Friday! DM me if you're interested!\n",
      "Gold: neutral\n",
      "Pred: positive\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mis_idx = np.where(y_dev_pred_baseline != y_dev)[0]\n",
    "\n",
    "print(\"First 3 misclassified dev examples (baseline):\\n\")\n",
    "for i in mis_idx[:3]:\n",
    "    row = dev_df.iloc[i]\n",
    "    print(\"Tweet ID:\", row[\"id\"])\n",
    "    print(\"Tweet:\", row[\"tweet\"])\n",
    "    print(\"Gold:\", id2sentiment[y_dev[i]])\n",
    "    print(\"Pred:\", id2sentiment[y_dev_pred_baseline[i]])\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d45f0a5d-31a3-4059-839f-44ce4b8a2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- IMPROVED SYSTEM ----------\n",
    "# Use TF-IDF, unigrams + bigrams, ignore very rare terms\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"\\b\\w+\\b\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_improved = tfidf_vectorizer.fit_transform(train_df[\"tweet\"])\n",
    "X_dev_improved   = tfidf_vectorizer.transform(dev_df[\"tweet\"])\n",
    "\n",
    "improved_clf = LinearSVC(C=1.0)  # you can tune this\n",
    "\n",
    "improved_clf.fit(X_train_improved, y_train)\n",
    "\n",
    "y_train_pred_improved = improved_clf.predict(X_train_improved)\n",
    "y_dev_pred_improved   = improved_clf.predict(X_dev_improved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "635a70b8-6911-45d1-a8f1-372af7629e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 4662\n",
      "sentiment\n",
      "neutral     2197\n",
      "positive    1495\n",
      "negative     970\n",
      "Name: count, dtype: int64\n",
      "Baseline test predictions shape: (4662,)\n",
      "Improved test predictions shape: (4662,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ---------- LOAD LABELLED TEST SET + FEATURES FOR BASELINE & IMPROVED ----------\n",
    "test_path = \"ttds_2025_cw2_test.txt\"   # <- your coursework test file\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\")\n",
    "\n",
    "# keep only the 3 sentiment labels we care about\n",
    "test_df = test_df[test_df[\"sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "print(\"Test size:\", len(test_df))\n",
    "print(test_df[\"sentiment\"].value_counts())\n",
    "\n",
    "# gold labels for test\n",
    "y_test = test_df[\"sentiment\"].map(sentiment2id).to_numpy()\n",
    "\n",
    "# ---------- baseline features for test (BOW with same vocab) ----------\n",
    "test_tokens = [tokenize(t) for t in test_df[\"tweet\"]]\n",
    "X_test_baseline = convert_to_bow_matrix(test_tokens, word2id)\n",
    "\n",
    "# ---------- improved features for test (TF-IDF, same vectorizer) ----------\n",
    "X_test_improved = tfidf_vectorizer.transform(test_df[\"tweet\"])\n",
    "\n",
    "# ---------- predictions ----------\n",
    "y_test_pred_baseline = baseline_clf.predict(X_test_baseline)\n",
    "y_test_pred_improved = improved_clf.predict(X_test_improved)\n",
    "\n",
    "print(\"Baseline test predictions shape:\", y_test_pred_baseline.shape)\n",
    "print(\"Improved test predictions shape:\", y_test_pred_improved.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a18a5f0-3cc9-41ac-b477-c3c33ef33a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a dict with all 12 scores in the required order:\n",
    "    p-pos,r-pos,f-pos,p-neg,r-neg,f-neg,p-neu,r-neu,f-neu,p-macro,r-macro,f-macro\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        sentiment2id[\"positive\"],\n",
    "        sentiment2id[\"negative\"],\n",
    "        sentiment2id[\"neutral\"],\n",
    "    ]\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    scores = {\n",
    "        \"p-pos\": p[0], \"r-pos\": r[0], \"f-pos\": f[0],\n",
    "        \"p-neg\": p[1], \"r-neg\": r[1], \"f-neg\": f[1],\n",
    "        \"p-neu\": p[2], \"r-neu\": r[2], \"f-neu\": f[2],\n",
    "        \"p-macro\": p.mean(),\n",
    "        \"r-macro\": r.mean(),\n",
    "        \"f-macro\": f.mean(),\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86437886-f48e-44e9-8920-6d7a3ffbf6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classification.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "def add_result_row(system_name, split_name, y_true, y_pred):\n",
    "    scores = compute_scores(y_true, y_pred)\n",
    "    row = {\"system\": system_name, \"split\": split_name}\n",
    "    row.update(scores)\n",
    "    results.append(row)\n",
    "\n",
    "# baseline\n",
    "add_result_row(\"baseline\", \"train\", y_train, y_train_pred_baseline)\n",
    "add_result_row(\"baseline\", \"dev\",   y_dev,   y_dev_pred_baseline)\n",
    "add_result_row(\"baseline\", \"test\",  y_test,  y_test_pred_baseline)\n",
    "\n",
    "# improved: train + dev\n",
    "add_result_row(\"improved\", \"train\", y_train, y_train_pred_improved)\n",
    "add_result_row(\"improved\", \"dev\",   y_dev,   y_dev_pred_improved)\n",
    "add_result_row(\"improved\", \"test\",  y_test,  y_test_pred_improved)\n",
    "\n",
    "# write CSV\n",
    "header = [\n",
    "    \"system\", \"split\",\n",
    "    \"p-pos\", \"r-pos\", \"f-pos\",\n",
    "    \"p-neg\", \"r-neg\", \"f-neg\",\n",
    "    \"p-neu\", \"r-neu\", \"f-neu\",\n",
    "    \"p-macro\", \"r-macro\", \"f-macro\"\n",
    "]\n",
    "\n",
    "with open(\"classification.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    # keep order: baseline train/dev/test, then improved train/dev/test\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Saved classification.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067055c6-8203-411f-9778-c5e5ec47f9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a67483-5162-45a5-a88b-419008c591ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
