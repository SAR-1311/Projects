{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024ed492-3ee9-49e4-81bf-f757d61ec1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, shlex\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2e9684-1e54-467b-9b36-46186c71b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TREC_FILE = \"trec.5000.xml\"\n",
    "STOPWORDS_FILE = \"stopwords.txt\"\n",
    "BOOLEAN_QUERIES_FILE = \"queries.boolean.txt\"\n",
    "RANKED_QUERIES_FILE = \"queries.ranked.txt\"\n",
    "RESULTS_FOLDER = \"results\"\n",
    "BOOLEAN_RESULTS_FILE = os.path.join(RESULTS_FOLDER, \"results.boolean.txt\")\n",
    "RANKED_RESULTS_FILE = os.path.join(RESULTS_FOLDER, \"results.ranked.txt\")\n",
    "INDEX_FILE = os.path.join(RESULTS_FOLDER, \"index.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c36eab5-6928-4516-b693-ed79835f6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_STOPWORDS = True   \n",
    "USE_STEMMING = True\n",
    "PROXIMITY_DEFAULT_K = 30\n",
    "TOP_K_RANKED = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3707a0e5-124e-40a4-aca9-e5b80cf61d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Stopwords enabled: True\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "ps = PorterStemmer()\n",
    "print(\"Configuration loaded. Stopwords enabled:\", USE_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905b4226-032e-4fc2-8eb5-b7ef4d9e65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(path):\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return set(w.strip().lower() for w in f if w.strip())\n",
    "\n",
    "STOPWORDS = load_stopwords(STOPWORDS_FILE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def preprocess(tokens):\n",
    "    if USE_STOPWORDS:\n",
    "        tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    if USE_STEMMING:\n",
    "        tokens = [ps.stem(t) for t in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761bdfe8-8882-43e4-b8c2-1030691ca74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 documents from trec.5000.xml\n"
     ]
    }
   ],
   "source": [
    "#LOADING THE XML FILE\n",
    "def load_trec_docs(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    docs = {}\n",
    "    for doc in root.findall('.//DOC'):\n",
    "        docno = doc.find('DOCNO').text.strip()\n",
    "        headline = doc.find('HEADLINE').text.strip() if doc.find('HEADLINE') is not None and doc.find('HEADLINE').text else ''\n",
    "        text = doc.find('TEXT').text.strip() if doc.find('TEXT') is not None and doc.find('TEXT').text else ''\n",
    "        content = f\"{headline} {text}\".strip()\n",
    "        docs[docno] = content\n",
    "    print(f\"Loaded {len(docs)} documents from {path}\")\n",
    "    return docs\n",
    "\n",
    "docs = load_trec_docs(TREC_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb79b32-1209-4e63-9da0-b6f4dbe91881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved to results\\index.txt\n",
      "Index built with 39308 unique terms.\n"
     ]
    }
   ],
   "source": [
    "#POSITIONAL INVERTED INDEX\n",
    "def build_index(docs):\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "    for docno, content in docs.items():\n",
    "        tokens = preprocess(tokenize(content))\n",
    "        for pos, term in enumerate(tokens, start=1):\n",
    "            index[term][docno].append(pos)\n",
    "    return index\n",
    "\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for term in sorted(index.keys()):\n",
    "            posting = index[term]\n",
    "            f.write(f\"{term}:{len(posting)}\\n\")\n",
    "            for docid, positions in posting.items():\n",
    "                f.write(f\"\\t{docid}: {', '.join(map(str, positions))}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(\"Index saved to\", INDEX_FILE)\n",
    "\n",
    "index = build_index(docs)\n",
    "save_index(index)\n",
    "print(\"Index built with\", len(index), \"unique terms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd30d41-0df9-4a97-ae42-269b79bc7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHRASE SEARCH\n",
    "def phrase_search(phrase, index):\n",
    "    tokens = preprocess(tokenize(phrase))\n",
    "    if not tokens or tokens[0] not in index:\n",
    "        return []\n",
    "    results = []\n",
    "    for doc in index[tokens[0]]:\n",
    "        for pos in index[tokens[0]][doc]:\n",
    "            if all((tokens[i] in index and (pos+i) in index[tokens[i]][doc]) for i in range(1, len(tokens))):\n",
    "                results.append(doc)\n",
    "                break\n",
    "    return sorted(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977366ca-bb3a-4c4a-ad78-571fbb8f1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROXIMITY SEARCH\n",
    "def proximity_search(t1, t2, k, index):\n",
    "    t1, t2 = preprocess([t1, t2])\n",
    "    if t1 not in index or t2 not in index:\n",
    "        return []\n",
    "    results = []\n",
    "    for doc in set(index[t1]) & set(index[t2]):\n",
    "        for p1 in index[t1][doc]:\n",
    "            if any(abs(p1 - p2) <= k for p2 in index[t2][doc]):\n",
    "                results.append(doc)\n",
    "                break\n",
    "    return sorted(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9af68b-afc1-49cf-b3d5-5bf476487e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOLEAN SEARCH\n",
    "def boolean_search(query, index):\n",
    "    query = query.replace(\"AND\", \"&\").replace(\"OR\", \"|\").replace(\"NOT\", \"-\")\n",
    "    phrase = re.findall(r'\"(.*?)\"', query)\n",
    "    if phrase:\n",
    "        return set(phrase_search(phrase[0], index))\n",
    "    prox = re.findall(r\"#(\\d+)\\(([^,]+),([^\\)]+)\\)\", query)\n",
    "    if prox:\n",
    "        k, t1, t2 = int(prox[0][0]), prox[0][1].strip(), prox[0][2].strip()\n",
    "        return set(proximity_search(t1, t2, k, index))\n",
    "    terms = preprocess(tokenize(query))\n",
    "    results = [set(index[t].keys()) for t in terms if t in index]\n",
    "    return set.intersection(*results) if results else set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff66f32f-3b2f-4073-94f1-6b0d2f2f14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANKED RETRIEVAL (TF-IDF)\n",
    "def compute_tfidf(index, N):\n",
    "    idf = {t: math.log10(N / len(index[t])) for t in index if len(index[t]) > 0}\n",
    "    doc_vecs = defaultdict(dict)\n",
    "    for t, posting in index.items():\n",
    "        for d, positions in posting.items():\n",
    "            tf = len(positions)\n",
    "            doc_vecs[d][t] = (1 + math.log10(tf)) * idf[t]\n",
    "    norms = {d: math.sqrt(sum(v*v for v in vec.values())) for d, vec in doc_vecs.items()}\n",
    "    return idf, doc_vecs, norms\n",
    "\n",
    "def ranked_search(query, index, idf, doc_vecs, norms):\n",
    "    tokens = preprocess(tokenize(query))\n",
    "    q_tf = Counter(tokens)\n",
    "    q_vec = {t: (1 + math.log10(f)) * idf.get(t, 0) for t, f in q_tf.items()}\n",
    "    q_norm = math.sqrt(sum(v*v for v in q_vec.values()))\n",
    "    scores = {}\n",
    "    for d in doc_vecs:\n",
    "        dot = sum(q_vec.get(t,0)*doc_vecs[d].get(t,0) for t in q_vec)\n",
    "        denom = norms[d]*q_norm\n",
    "        if denom > 0:\n",
    "            scores[d] = dot/denom\n",
    "    return sorted(scores.items(), key=lambda x:x[1], reverse=True)[:TOP_K_RANKED]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9916cae7-7699-4f91-bc1a-af9a1f4384a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running IR system...\n",
      "Results saved to results\\results.boolean.txt\n",
      "Results saved to results\\results.ranked.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\nRunning IR system...\")\n",
    "    idf, doc_vecs, norms = compute_tfidf(index, len(docs))\n",
    "\n",
    "    #BOOLEAN & PHRASE & PROXIMITY \n",
    "    with open(BOOLEAN_QUERIES_FILE, 'r', encoding='utf-8') as f:\n",
    "        queries = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    with open(BOOLEAN_RESULTS_FILE, 'w', encoding='utf-8') as out:\n",
    "        for qline in queries:\n",
    "            qnum, *qterms = qline.split()\n",
    "            qtext = \" \".join(qterms)\n",
    "            results = boolean_search(qtext, index)\n",
    "            for doc in sorted(results):\n",
    "                out.write(f\"{qnum},{doc}\\n\")\n",
    "    print(f\"Results saved to {BOOLEAN_RESULTS_FILE}\")\n",
    "\n",
    "    #RANKED RETRIEVAL (TF-IDF)\n",
    "    with open(RANKED_QUERIES_FILE, 'r', encoding='utf-8') as f:\n",
    "        queries = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    with open(RANKED_RESULTS_FILE, 'w', encoding='utf-8') as out:\n",
    "        for qline in queries:\n",
    "            qnum, *qterms = qline.split()\n",
    "            qtext = \" \".join(qterms)\n",
    "            ranked = ranked_search(qtext, index, idf, doc_vecs, norms)\n",
    "            for doc, score in ranked:\n",
    "                out.write(f\"{qnum},{doc},{score:.4f}\\n\")\n",
    "    print(f\"Results saved to {RANKED_RESULTS_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcb31b-5455-48b6-bde8-09e476b65913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
